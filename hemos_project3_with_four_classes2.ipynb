{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90_fM8DhbZLQ",
        "outputId": "e4d2da4a-6525-42fa-895d-3a98463ad9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (0.42.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.353 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.353 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ],
      "source": [
        "!pip install jieba\n",
        "import pandas as pd\n",
        "import jieba\n",
        "\n",
        "# Load your CSV file\n",
        "weibo_df = pd.read_csv('weibo_senti_100k_utf8_filtered_four_classes.csv')\n",
        "\n",
        "# Define a function to segment text\n",
        "def segment_text(text):\n",
        "    return ' '.join(jieba.cut(text, cut_all=False))\n",
        "\n",
        "# Apply the text segmentation function to the 'review_cleaned' column\n",
        "weibo_df['review_cutted_project'] = weibo_df['review_cleaned'].apply(segment_text)\n",
        "\n",
        "# Save the DataFrame back to CSV\n",
        "weibo_df.to_csv('weibo_senti_100k_utf8_filtered_four_classes_processed.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weibo_df['review_cutted_project'][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6YUCt-AQD2J",
        "outputId": "bb8817c1-7eda-4ae3-f4cd-843750d85fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         更博 了 爆照 了 帅 的 呀 就是 越来越 爱 你 生快 傻 缺爱 你 爱 你 爱 你\n",
              "1      土耳其 的 事要 认真对待 哈哈 否则 直接 开除   很 是 细心 酒店 都 全部 OK 啦\n",
              "2                                姑娘 都 羡慕 你 呢 还有 招财猫 高兴\n",
              "3                                                 美爱 你\n",
              "4                               梦想 有 多 大 舞台 就 有 多 大 鼓掌\n",
              "5                                         花心 鼓掌   春暖花开\n",
              "6    某 问答 社区 上 收到 一 大学生 发给 我 的 私信 偶 喜欢 阿姨 偶是 阿姨 控 我...\n",
              "7    吃货 们 无不 啧啧称奇 好 不 喜欢 PS 写错 一个 字 哈哈                \n",
              "8    Sweet   MorningFrom   now   onlove   yourselfe...\n",
              "9    霍思燕 剖腹产 下小江江   老公 落泪 今晨 9 时 霍思燕 产下 一名 男婴 宝宝 重 ...\n",
              "Name: review_cutted_project, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras sklearn jieba\n",
        "\n",
        "import pandas as pd\n",
        "import jieba\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Attention, Lambda\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw0R5SxLgpMv",
        "outputId": "37cf541a-912c-4c4f-f9a6-145a667ba53a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post11.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('weibo_senti_100k_utf8_filtered_four_classes_processed.csv')\n",
        "\n",
        "# Segment text with jieba\n",
        "df['review_cutted_project'] = df['review_cleaned'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
        "\n",
        "# Preprocessing (assuming the data is already loaded and balanced in df)\n",
        "X = df['review_cutted_project'].astype(str)\n",
        "y = df['sentiment_classified_four_classes']\n",
        "# 1=positive 2=negative 3=Optimistic humorous 4=Pessimistic humorous\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Calculate max sequence length\n",
        "max_seq_length = max([len(x) for x in X_seq])\n",
        "\n",
        "X_pad = pad_sequences(X_seq, maxlen=max([len(x) for x in X_seq]))\n",
        "\n",
        "# Encode class values\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y_enc = encoder.transform(y)\n",
        "y_dummy = to_categorical(y_enc)\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_dummy, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XRGBGX54cwVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# Resampling with SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Converting the one-hot encoded y_train_resampled back to a 1D array of class labels\n",
        "y_train_labels = np.argmax(y_train_resampled, axis=1)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
        "class_weights_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "w35yNJdcc-YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class_distribution_before = Counter(y_train)\n",
        "\n",
        "# Display the class distribution\n",
        "print(\"Class distribution after before SMOTE:\", class_distribution_before)\n",
        "\n",
        "# Optionally, you can calculate and display the ratio\n",
        "total_samples = sum(class_distribution_before.values())\n",
        "class_ratios = {cls: count / total_samples for cls, count in class_distribution_before.items()}\n",
        "\n",
        "print(\"Class ratios after before SMOTE:\", class_ratios)\n",
        "\n",
        "# Count the occurrences of each class in the resampled dataset\n",
        "class_distribution = Counter(y_train_resampled)\n",
        "\n",
        "# Display the class distribution\n",
        "print(\"Class distribution after applying SMOTE:\", class_distribution)\n",
        "\n",
        "# Optionally, you can calculate and display the ratio\n",
        "total_samples = sum(class_distribution.values())\n",
        "class_ratios = {cls: count / total_samples for cls, count in class_distribution.items()}\n",
        "\n",
        "print(\"Class ratios after applying SMOTE:\", class_ratios)"
      ],
      "metadata": {
        "id": "kX9IDnTVdEMr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "0f15a950-746f-4a09-e7e3-7bb837e8464c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fa95208311d8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclass_distribution_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Display the class distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m         '''\n\u001b[1;32m    576\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    668\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for the model (adjust these to suit your data)\n",
        "max_features = 20000  # Adjust to the size of your vocabulary\n",
        "maxlen = 100          # Adjust to the length that makes sense for your data\n",
        "embedding_dim = 128\n",
        "lstm_units = 64\n",
        "\n",
        "# Model building\n",
        "input_layer = Input(shape=(maxlen,))\n",
        "embedding_layer = Embedding(max_features, embedding_dim, input_length=maxlen)(input_layer)\n",
        "bilstm_layer = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedding_layer)\n",
        "attention_layer = Attention()([bilstm_layer, bilstm_layer])\n",
        "lambda_layer = Lambda(lambda x: K.mean(x, axis=1))(attention_layer)\n",
        "dense_layer = Dense(64, activation='relu')(lambda_layer)\n",
        "dropout_layer = Dropout(0.5)(dense_layer)\n",
        "output_layer = Dense(4, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3HUDuFxHc4NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_split=0.1, callbacks=[early_stopping], verbose=1)"
      ],
      "metadata": {
        "id": "qyJH4mNLdQbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "y_pred = model.predict(X_test).argmax(axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Reverse transform from encoded labels to original labels\n",
        "y_pred = encoder.inverse_transform(y_pred)\n",
        "y_true = encoder.inverse_transform(y_true)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred, target_names=encoder.classes_.astype(str))\n",
        "print(report)"
      ],
      "metadata": {
        "id": "TOx2WXMTdiv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "edq7HTzqWJlB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}